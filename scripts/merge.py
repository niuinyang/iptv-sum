import os
import re
import csv
import requests
from collections import defaultdict

# ==============================
# é…ç½®è·¯å¾„
# ==============================
SOURCES_FILE = "input/network/networksource.txt"  # æ¯è¡Œä¸€ä¸ªæºåœ°å€ï¼ˆæœ¬åœ°æ–‡ä»¶æˆ– URLï¼‰
OUTPUT_DIR = "output"
LOG_DIR = os.path.join(OUTPUT_DIR, "log")
MIDDLE_DIR = os.path.join(OUTPUT_DIR, "middle")

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(MIDDLE_DIR, exist_ok=True)

OUTPUT_M3U = os.path.join(OUTPUT_DIR, "merge_total.m3u")
OUTPUT_CSV = os.path.join(OUTPUT_DIR, "total.csv")
SKIPPED_FILE = os.path.join(LOG_DIR, "skipped.log")

HEADERS = {"User-Agent": "Mozilla/5.0"}
RETRY_TIMES = 3
TIMEOUT = 15

# ==============================
# è·å–æºæ–‡ä»¶å†…å®¹ï¼ˆå¢å¼ºç‰ˆï¼‰
# ==============================
def fetch_sources(file_path):
    all_lines = []
    success, failed = 0, 0

    with open(file_path, "r", encoding="utf-8") as f:
        urls = [u.strip() for u in f if u.strip() and not u.startswith("#")]

    for url in urls:
        print(f"ğŸ“¡ Fetching: {url}")
        try:
            if url.startswith("http"):
                text = None
                for attempt in range(RETRY_TIMES):
                    try:
                        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
                        r.raise_for_status()
                        # å°è¯•å¤šç§ç¼–ç 
                        for enc in [r.encoding, r.apparent_encoding, "utf-8", "utf-8-sig", "latin1"]:
                            try:
                                text = r.content.decode(enc)
                                break
                            except:
                                continue
                        if text is None:
                            raise Exception("æ— æ³•è§£ç å†…å®¹")

                        # ç®€å•è¿‡æ»¤ HTML é¡µé¢
                        if "<html" in text.lower() and "<body" in text.lower():
                            raise Exception("å†…å®¹ç–‘ä¼¼ HTML é¡µé¢ï¼Œé M3U")

                        break
                    except Exception as e:
                        print(f"âš ï¸ Retry {attempt+1}/{RETRY_TIMES} failed: {e}")
                if text is None:
                    raise Exception("Failed after retries")
            else:
                # æœ¬åœ°æ–‡ä»¶å°è¯•å¤šç§ç¼–ç 
                text = None
                for enc in ["utf-8", "utf-8-sig", "latin1"]:
                    try:
                        with open(url, encoding=enc, errors="ignore") as f_local:
                            text = f_local.read()
                        break
                    except Exception as e:
                        continue
                if text is None:
                    raise Exception("æ— æ³•è¯»å–æœ¬åœ°æ–‡ä»¶")

            # æ‹†è¡Œ
            lines = text.splitlines()
            print(f"æº {url} å…± {len(lines)} è¡Œï¼Œå‰ 5 è¡Œé¢„è§ˆ: {lines[:5]}")

            # å»æ‰ #EXTM3U
            filtered_lines = []
            removed_header = False
            for l in lines:
                l_strip = l.strip()
                if l_strip.startswith("#EXTM3U") and not removed_header:
                    removed_header = True
                    continue
                if l_strip:
                    filtered_lines.append(l_strip)

            print(f"è¿‡æ»¤å {len(filtered_lines)} è¡Œ")
            all_lines.extend(filtered_lines)
            success += 1
        except Exception as e:
            failed += 1
            with open(SKIPPED_FILE, "a", encoding="utf-8") as f_log:
                f_log.write(f"âŒ Failed: {url} ({e})\n")
            print(f"âŒ Failed: {url} ({e})")

    return all_lines, success, failed

# ==============================
# è§£æ EXTINF + URL å¯¹
# ==============================
def parse_channels(lines):
    pairs = []
    for i, line in enumerate(lines):
        if line.startswith("#EXTINF"):
            # å‘ä¸‹æ‰¾ç¬¬ä¸€ä¸ª URL
            for j in range(i+1, len(lines)):
                url_line = lines[j].strip()
                if url_line.startswith("http"):
                    pairs.append((line, url_line))
                    break
    return pairs

# ==============================
# å»é‡ EXTINF + URL
# ==============================
def deduplicate(pairs):
    seen = set()
    unique_pairs = []
    for title, url in pairs:
        key = (title, url)
        if key not in seen:
            unique_pairs.append((title, url))
            seen.add(key)
    return unique_pairs

# ==============================
# è‡ªç„¶æ’åº
# ==============================
def natural_sort_key(text):
    return [int(t) if t.isdigit() else t.lower() for t in re.split(r"([0-9]+)", text)]

# ==============================
# åˆ†ç»„æ’åº
# ==============================
def group_sort(pairs):
    group_dict = defaultdict(list)
    group_pattern = re.compile(r'group-title="([^"]*)"')

    for title, url in pairs:
        match = group_pattern.search(title)
        group_name = match.group(1).strip() if match else "æœªåˆ†ç±»"
        group_dict[group_name].append((title, url))

    # åˆ†ç»„æ’åºï¼Œç»„å†…è‡ªç„¶æ’åº
    sorted_pairs = []
    for group in sorted(group_dict.keys()):
        group_items = group_dict[group]
        group_items.sort(key=lambda x: natural_sort_key(x[0]))
        sorted_pairs.extend(group_items)
    return sorted_pairs

# ==============================
# å†™å…¥ total.m3u
# ==============================
def write_m3u(pairs, output_file):
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for title, url in pairs:
            f.write(f"{title}\n{url}\n")

# ==============================
# å†™å…¥ CSV
# ==============================
def write_csv(pairs, csv_file):
    with open(csv_file, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["title", "url"])
        for title, url in pairs:
            writer.writerow([title, url])

# ==============================
# ä¸»æµç¨‹
# ==============================
if __name__ == "__main__":
    # æ¸…ç©ºæ—¥å¿—
    if os.path.exists(SKIPPED_FILE):
        os.remove(SKIPPED_FILE)

    all_lines, success, failed = fetch_sources(SOURCES_FILE)
    if not all_lines:
        print("âš ï¸ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•å†…å®¹ï¼Œè¯·æ£€æŸ¥ networksource.txt æˆ–ç½‘ç»œè¿æ¥")
    parsed_pairs = parse_channels(all_lines)

    # å†™å…¥ä¸­é—´ CSV/M3U æ–‡ä»¶
    write_csv(parsed_pairs, os.path.join(MIDDLE_DIR, "parsed.csv"))
    write_m3u(parsed_pairs, os.path.join(MIDDLE_DIR, "parsed.m3u"))

    unique_pairs = deduplicate(parsed_pairs)
    grouped_sorted_pairs = group_sort(unique_pairs)

    write_m3u(grouped_sorted_pairs, OUTPUT_M3U)
    write_csv(grouped_sorted_pairs, OUTPUT_CSV)

    print(f"\nâœ… åˆå¹¶å®Œæˆï¼šæˆåŠŸ {success} æºï¼Œå¤±è´¥ {failed} æºï¼Œ"
          f"å»é‡å {len(grouped_sorted_pairs)} æ¡é¢‘é“ â†’ {OUTPUT_M3U} / {OUTPUT_CSV}")
    print(f"ğŸ“ ä¸­é—´æ–‡ä»¶ â†’ {MIDDLE_DIR}")
    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶ â†’ {SKIPPED_FILE}")