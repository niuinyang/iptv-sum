import aiohttp
import asyncio
import os
from aiohttp import ClientSession, ClientTimeout
from datetime import datetime
import random

# ==============================
# é…ç½®åŒº
# ==============================
SOURCE_LIST_FILE = "input/network/networksource.txt"  # åŒ…å«æ‰€æœ‰æºçš„åˆ—è¡¨
SAVE_DIR = "input/network/network_sources"            # ä¸‹è½½ä¿å­˜è·¯å¾„
MAX_CONCURRENT = 5                                    # å¹¶å‘ä¸‹è½½æ•°
RETRY_COUNT = 3                                       # é‡è¯•æ¬¡æ•°

# æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) Gecko/20100101 Firefox/120.0",
]

HEADERS = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
}

# ==============================
# åˆ›å»ºæ–‡ä»¶å¤¹
# ==============================
os.makedirs(SAVE_DIR, exist_ok=True)


# ==============================
# å¼‚æ­¥ä¸‹è½½å‡½æ•°
# ==============================
async def fetch(session: ClientSession, url: str, retries=RETRY_COUNT):
    for attempt in range(1, retries + 1):
        try:
            headers = HEADERS.copy()
            headers["User-Agent"] = random.choice(USER_AGENTS)

            async with session.get(url, headers=headers) as resp:
                if resp.status == 200:
                    text = await resp.text()
                    return text
                else:
                    print(f"âš ï¸ [{resp.status}] {url}")
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {attempt}/{retries} æ¬¡é‡è¯•å¤±è´¥ï¼š{url} ({e})")
            await asyncio.sleep(2 * attempt)
    return None


# ==============================
# ä¸‹è½½ä»»åŠ¡
# ==============================
async def download_all():
    # è¯»å–æºåˆ—è¡¨
    if not os.path.exists(SOURCE_LIST_FILE):
        print(f"âŒ æœªæ‰¾åˆ°æºåˆ—è¡¨æ–‡ä»¶: {SOURCE_LIST_FILE}")
        return

    with open(SOURCE_LIST_FILE, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    if not urls:
        print("âŒ æºåˆ—è¡¨ä¸ºç©ºï¼Œé€€å‡ºã€‚")
        return

    print(f"ğŸ“¡ å…± {len(urls)} ä¸ªæºï¼Œå°†ä¿å­˜åˆ° {SAVE_DIR}")

    timeout = ClientTimeout(total=30)
    connector = aiohttp.TCPConnector(limit_per_host=MAX_CONCURRENT)
    async with ClientSession(timeout=timeout, connector=connector) as session:
        tasks = []
        for url in urls:
            tasks.append(download_one(session, url))
        await asyncio.gather(*tasks)


# ==============================
# ä¸‹è½½å•ä¸ªæº
# ==============================
async def download_one(session, url):
    filename = os.path.basename(url.split("?")[0])
    if not filename.endswith(".m3u"):
        filename += ".m3u"

    save_path = os.path.join(SAVE_DIR, filename)
    print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½: {url}")

    content = await fetch(session, url)
    if content:
        with open(save_path, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"âœ… ä¿å­˜æˆåŠŸ: {save_path} ({len(content)} å­—èŠ‚)")
    else:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {url}")


# ==============================
# ä¸»å…¥å£
# ==============================
if __name__ == "__main__":
    print(f"ğŸš€ IPTV æºä¸‹è½½å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    asyncio.run(download_all())
    print(f"ğŸ IPTV æºä¸‹è½½å®Œæˆ: {SAVE_DIR}")