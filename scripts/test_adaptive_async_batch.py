import requests, os, time, json, re
from concurrent.futures import ThreadPoolExecutor, as_completed
from statistics import mean
from collections import defaultdict
import multiprocessing

# ==============================
# é…ç½®åŒº
# ==============================
input_file = "output/total.m3u"
output_file = "output/working.m3u"
progress_file = "output/progress.json"
os.makedirs("output", exist_ok=True)

TIMEOUT = 10
BASE_THREADS = 50
MAX_THREADS = 200
BATCH_SIZE = 300

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0 Safari/537.36",
    "Accept": "*/*",
    "Connection": "keep-alive",
}

# ==============================
# ä½åˆ†è¾¨ç‡å’Œå…³é”®å­—è¿‡æ»¤
# ==============================
LOW_RES_KEYWORDS = ["SD", "VGA", "480p", "576p"]
BLOCK_KEYWORDS = ["espanol"]  # ä¸æ£€æµ‹è¿™äº›å…³é”®å­—

def is_high_res(title):
    return not any(kw.lower() in title.lower() for kw in LOW_RES_KEYWORDS)

def is_allowed(title, url):
    """æ˜¯å¦å…è®¸æ£€æµ‹ï¼šé«˜æ¸…ä¸”ä¸å«é»‘åå•å…³é”®å­—"""
    if not is_high_res(title):
        return False
    for kw in BLOCK_KEYWORDS:
        if kw.lower() in title.lower() or kw.lower() in url.lower():
            return False
    return True

# ==============================
# æ£€æµ‹å‡½æ•°
# ==============================
def quick_check(url):
    try:
        r = requests.head(url, headers=HEADERS, timeout=TIMEOUT, allow_redirects=True)
        if r.status_code < 400 and (
            "video" in r.headers.get("content-type", "").lower()
            or url.lower().endswith((".m3u8", ".ts"))
        ):
            return True
    except:
        pass
    return False

def deep_check(url):
    try:
        r = requests.get(url, headers=HEADERS, stream=True, timeout=TIMEOUT)
        for _ in range(3):
            chunk = next(r.iter_content(chunk_size=8192), b'')
            if any(sig in chunk for sig in [
                b"#EXTM3U", b"mpegts", b"ftyp", b"\x00\x00\x01\xb3", b"HTTP Live Streaming"
            ]):
                return True
            if not chunk:
                break
    except:
        pass
    return False

def test_stream(url):
    start = time.time()
    ok = quick_check(url) or deep_check(url)
    elapsed = round(time.time() - start, 3)
    return ok, elapsed

def detect_optimal_threads():
    test_urls = [
        "https://www.apple.com",
        "https://www.google.com",
        "https://www.microsoft.com",
    ]
    times = []
    for u in test_urls:
        t0 = time.time()
        try:
            requests.head(u, timeout=TIMEOUT)
        except:
            pass
        times.append(time.time() - t0)
    avg = mean(times)
    cpu_threads = multiprocessing.cpu_count() * 5
    if avg < 0.5:
        return min(MAX_THREADS, cpu_threads)
    elif avg < 1:
        return min(150, cpu_threads)
    elif avg < 2:
        return min(100, cpu_threads)
    else:
        return BASE_THREADS

# ==============================
# è¯»å– M3U å¹¶ç”Ÿæˆ (title, url) å¯¹
# ==============================
lines = open(input_file, encoding="utf-8").read().splitlines()
pairs = []
i = 0
while i < len(lines):
    if lines[i].startswith("#EXTINF") and i + 1 < len(lines):
        title, url = lines[i], lines[i+1]
        if is_allowed(title, url):
            pairs.append((title, url))
        i += 2
    else:
        i += 1

# ==============================
# æ¢å¤è¿›åº¦
# ==============================
done_index = 0
if os.path.exists(progress_file):
    try:
        done_index = json.load(open(progress_file, encoding="utf-8")).get("done", 0)
        print(f"ğŸ”„ æ¢å¤è¿›åº¦ï¼Œä»ç¬¬ {done_index} æ¡ç»§ç»­")
    except:
        pass

total = len(pairs)
threads = detect_optimal_threads()
print(f"âš™ï¸ åŠ¨æ€å¹¶å‘çº¿ç¨‹æ•°ï¼š{threads}")
print(f"ğŸš€ å¼€å§‹æ£€æµ‹ {total} æ¡ç¬¦åˆæ¡ä»¶çš„æµï¼ˆæ¯æ‰¹ {BATCH_SIZE} æ¡ï¼‰")

start_time = time.time()
all_working = []

# ==============================
# æ‰¹é‡æ£€æµ‹
# ==============================
for batch_start in range(done_index, total, BATCH_SIZE):
    batch = pairs[batch_start: batch_start + BATCH_SIZE]
    working_batch = []

    with ThreadPoolExecutor(max_workers=threads) as executor:
        futures = {executor.submit(test_stream, url): (title, url) for title, url in batch}
        for future in as_completed(futures):
            title, url = futures[future]
            try:
                ok, elapsed = future.result()
                if ok:
                    working_batch.append((title, url, elapsed))
            except:
                pass

    all_working.extend(working_batch)

    # æ›´æ–°è¿›åº¦æ–‡ä»¶
    json.dump({"done": min(batch_start + BATCH_SIZE, total)}, open(progress_file, "w", encoding="utf-8"))

    print(f"ğŸ§® æœ¬æ‰¹å®Œæˆï¼š{len(working_batch)}/{len(batch)} å¯ç”¨æµ | å·²å®Œæˆ {min(batch_start + BATCH_SIZE, total)}/{total}")

# åˆ é™¤è¿›åº¦æ–‡ä»¶
if os.path.exists(progress_file):
    os.remove(progress_file)

# ==============================
# æŒ‰é¢‘é“åˆ†ç»„å¹¶æŒ‰å“åº”é€Ÿåº¦æ’åº
# ==============================
grouped = defaultdict(list)
for title, url, elapsed in all_working:
    # å°è¯•æå–é¢‘é“å
    m = re.search(r'[,](.+)$', title)
    channel_name = m.group(1).strip() if m else title
    grouped[channel_name].append((title, url, elapsed))

# ç»„å†…æŒ‰å“åº”é€Ÿåº¦æ’åº
for name in grouped:
    grouped[name].sort(key=lambda x: x[2])

# å†™å…¥è¾“å‡ºæ–‡ä»¶
with open(output_file, "w", encoding="utf-8") as f:
    f.write("#EXTM3U\n")
    for name in sorted(grouped.keys()):  # æŒ‰é¢‘é“åæ’åº
        for title, url, _ in grouped[name]:
            f.write(f"{title}\n{url}\n")

elapsed_total = round(time.time() - start_time, 2)
print(f"âœ… æ£€æµ‹å®Œæˆï¼Œå…± {len(all_working)} æ¡å¯ç”¨é«˜æ¸…åŠä»¥ä¸Šæµï¼Œç”¨æ—¶ {elapsed_total} ç§’")