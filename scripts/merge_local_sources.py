import os
import csv
import re
from collections import defaultdict

# ==============================
# ç»å¯¹è·¯å¾„é…ç½®
# ==============================
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
SOURCE_DIR = os.path.join(ROOT_DIR, "input/network/network_sources")
OUTPUT_DIR = os.path.join(ROOT_DIR, "output")
LOG_DIR = os.path.join(OUTPUT_DIR, "log")

MERGE_M3U = os.path.join(OUTPUT_DIR, "merge_total.m3u")
MERGE_CSV = os.path.join(OUTPUT_DIR, "merge_total.csv")
SKIP_LOG = os.path.join(LOG_DIR, "skipped.log")

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# ==============================
# åŠŸèƒ½å‡½æ•°
# ==============================

def normalize_channel_name(name: str) -> str:
    """æ ‡å‡†åŒ–é¢‘é“å"""
    name = re.sub(r'\s*\(.*?\)|\[.*?\]', '', name)
    name = re.sub(r'[^0-9A-Za-z\u4e00-\u9fa5]+', '', name)
    return name.strip().lower()

def parse_m3u(file_path):
    """è§£æ M3U æ–‡ä»¶ä¸ºé¢‘é“åˆ—è¡¨"""
    channels = []
    try:
        with open(file_path, encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()
        for i in range(len(lines)):
            if lines[i].startswith("#EXTINF:"):
                info = lines[i].strip()
                url = lines[i + 1].strip() if i + 1 < len(lines) else ""
                name_match = re.search(r',(.+)$', info)
                name = name_match.group(1).strip() if name_match else "æœªçŸ¥é¢‘é“"
                channels.append((name, url))
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {file_path} ({e})")
    return channels

# ==============================
# ä¸»é€»è¾‘
# ==============================

def main():
    all_channels = defaultdict(set)
    skipped = []

    print(f"ğŸ“‚ æ­£åœ¨è¯»å–æ–‡ä»¶å¤¹: {SOURCE_DIR}")

    for file in os.listdir(SOURCE_DIR):
        if not file.endswith(".m3u"):
            continue
        path = os.path.join(SOURCE_DIR, file)
        channels = parse_m3u(path)
        print(f"ğŸ“¡ å·²åŠ è½½ {file}: {len(channels)} æ¡é¢‘é“")
        for name, url in channels:
            norm_name = normalize_channel_name(name)
            if not url.startswith("http"):
                skipped.append((name, url))
                continue
            all_channels[norm_name].add((name, url))

    merged_channels = []
    for ch_name, items in all_channels.items():
        # å–ç¬¬ä¸€ä¸ªéç©ºæº
        for name, url in items:
            merged_channels.append((name, url))
            break

    # è¾“å‡º M3U
    with open(MERGE_M3U, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for name, url in merged_channels:
            f.write(f"#EXTINF:-1,{name}\n{url}\n")

    # è¾“å‡º CSV
    with open(MERGE_CSV, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["name", "url"])
        for name, url in merged_channels:
            writer.writerow([name, url])

    # è·³è¿‡æ—¥å¿—
    with open(SKIP_LOG, "w", encoding="utf-8") as f:
        for name, url in skipped:
            f.write(f"{name} | {url}\n")

    print(f"\nâœ… åˆå¹¶å®Œæˆï¼šå…± {len(merged_channels)} æ¡é¢‘é“")
    print(f"ğŸ“ è¾“å‡º M3U: {MERGE_M3U}")
    print(f"ğŸ“ è¾“å‡º CSV: {MERGE_CSV}")
    print(f"ğŸ“ è·³è¿‡æ—¥å¿—: {SKIP_LOG}")

if __name__ == "__main__":
    main()