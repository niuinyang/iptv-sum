import os
import re
import csv
import unicodedata
from urllib.parse import urlparse

# ==============================
# é…ç½®åŒº
# ==============================
SOURCE_DIR = "input/network/network_sources"  # ä¸‹è½½æºç›®å½•
OUTPUT_DIR = "output"
LOG_DIR = os.path.join(OUTPUT_DIR, "log")

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

OUTPUT_M3U = os.path.join(OUTPUT_DIR, "merge_total.m3u")
OUTPUT_CSV = os.path.join(OUTPUT_DIR, "merge_total.csv")
SKIPPED_LOG = os.path.join(LOG_DIR, "skipped.log")

# ==============================
# å·¥å…·å‡½æ•°
# ==============================
def normalize_channel_name(name: str) -> str:
    """æ ‡å‡†åŒ–é¢‘é“åï¼ˆå»æ‰ç¬¦å·ã€ç©ºæ ¼ã€å¤§å°å†™ç»Ÿä¸€ï¼‰"""
    name = unicodedata.normalize("NFKC", name or "")
    name = re.sub(r"[\s\[\]ï¼ˆï¼‰()ã€ã€‘]", "", name)
    name = re.sub(r"[-_\.]", "", name)
    return name.strip().lower()

def fallback_name_from_url(url: str) -> str:
    try:
        p = urlparse(url)
        host = p.hostname or url
        return host.split('.')[-2] if host and '.' in host else host
    except:
        return url

def read_m3u_file(file_path: str):
    """
    è¯»å– M3U æ–‡ä»¶ï¼Œè¿”å› (é¢‘é“å, URL) åˆ—è¡¨
    æ›´é²æ£’çš„è§£æï¼šä¼˜å…ˆå– tvg-nameï¼Œè‹¥æ— åˆ™å– EXTINF é€—å·åçš„æ˜¾ç¤ºåï¼Œè‹¥ä»æ— åˆ™é€€å› URL ä¸»æœºå
    """
    channels = []
    try:
        with open(file_path, "r", encoding="utf-8", errors="replace") as f:
            lines = [ln.rstrip("\n") for ln in f]

        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            # æ‰¾åˆ° #EXTINF è¡Œ
            if line.startswith("#EXTINF"):
                info_line = line

                # 1) å°è¯•ç”¨ tvg-name="..."`
                m_name = re.search(r'tvg-name\s*=\s*"([^"]+)"', info_line, re.IGNORECASE)
                title = m_name.group(1).strip() if m_name else None

                # 2) è‹¥æ²¡æœ‰ tvg-nameï¼Œåˆ™å°è¯•å– #EXTINF:...,(æ˜¾ç¤ºå) çš„é€—å·åé¢éƒ¨åˆ†ï¼ˆå–æœ€åä¸€ä¸ªé€—å·ä¹‹åï¼‰
                if not title:
                    m_tail = re.search(r'#EXTINF:[^\n]*,(.*)$', info_line, re.IGNORECASE)
                    if m_tail:
                        title = m_tail.group(1).strip()

                # 3) è‹¥ä»æ— ï¼Œå°è¯• tvg-id æˆ– tvg-logo ç­‰å­—æ®µä½œä¸ºå¤‡ç”¨åï¼ˆå–ç¬¬ä¸€ä¸ªéç©ºï¼‰
                if not title:
                    m_id = re.search(r'tvg-id\s*=\s*"([^"]+)"', info_line, re.IGNORECASE)
                    if m_id:
                        title = m_id.group(1).strip()

                # 4) è·å–åé¢çš„ url è¡Œï¼ˆè·³è¿‡ç©ºè¡Œæˆ–æ³¨é‡Šï¼‰
                url_line = ""
                j = i + 1
                while j < len(lines):
                    candidate = lines[j].strip()
                    if candidate == "" or candidate.startswith("#"):
                        j += 1
                        continue
                    url_line = candidate
                    break

                # 5) å¦‚æœæ²¡æœ‰ titleï¼ˆæå°‘è§ï¼‰ï¼Œä» url æå–ä¸€ä¸ª fallback å
                if not title:
                    title = fallback_name_from_url(url_line) or "æœªçŸ¥é¢‘é“"

                # 6) æœ€ååŠ å…¥ï¼ˆåªæ¥å— http/httpsï¼‰
                if url_line and url_line.lower().startswith(("http://", "https://")):
                    channels.append((title, url_line))
                else:
                    # é http é“¾æ¥ä¹Ÿè®°å½•ä¸ºè·³è¿‡ç”¨æ—¥å¿—ï¼ˆä¿ç•™åŸ nameï¼‰
                    channels.append((title, url_line))
                # ç§»åŠ¨ç´¢å¼•åˆ° url è¡Œä¹‹å
                i = j + 1
            else:
                i += 1

        print(f"ğŸ“¡ å·²åŠ è½½ {os.path.basename(file_path)}: {len(channels)} æ¡é¢‘é“")
        return channels

    except Exception as e:
        print(f"âš ï¸ è¯»å– {file_path} å¤±è´¥: {e}")
        return []

# ==============================
# ä¸»é€»è¾‘ï¼ˆå»é‡ç›¸åŒ URLï¼‰
# ==============================
def merge_local_sources():
    all_channels = []
    skipped = []
    seen_urls = set()  # è®°å½•å·²å‡ºç°çš„ URL

    print(f"ğŸ“‚ æ­£åœ¨è¯»å–æ–‡ä»¶å¤¹: {os.path.abspath(SOURCE_DIR)}")

    for file in os.listdir(SOURCE_DIR):
        if not file.endswith(".m3u"):
            continue
        file_path = os.path.join(SOURCE_DIR, file)
        channels = read_m3u_file(file_path)

        for name, url in channels:
            # å¦‚æœæ²¡æœ‰ URL æˆ–ä¸æ˜¯ http(s)ï¼Œè®°å½•åˆ° skipped å¹¶è·³è¿‡
            if not url or not url.lower().startswith("http"):
                skipped.append((name, url))
                continue
            # å»é™¤ç›¸åŒ URL çš„é‡å¤æº
            if url in seen_urls:
                continue
            seen_urls.add(url)
            all_channels.append((name, url))

    print(f"\nâœ… åˆå¹¶å®Œæˆï¼šå…± {len(all_channels)} æ¡é¢‘é“ï¼ˆå·²å»é‡ç›¸åŒ URLï¼‰")
    print(f"ğŸ“ è¾“å‡º M3U: {OUTPUT_M3U}")
    print(f"ğŸ“ è¾“å‡º CSV: {OUTPUT_CSV}")

    # ==============================
    # å†™å…¥ M3U
    # ==============================
    with open(OUTPUT_M3U, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for name, url in all_channels:
            # å°½é‡ä¿ç•™åŸå§‹æ˜¾ç¤ºå
            f.write(f'#EXTINF:-1 tvg-name="{name}",{name}\n{url}\n')

    # ==============================
    # å†™å…¥ CSV
    # ==============================
    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["tvg-name", "URL"])
        writer.writerows(all_channels)

    # ==============================
    # å†™å…¥è·³è¿‡æ—¥å¿—
    # ==============================
    with open(SKIPPED_LOG, "w", encoding="utf-8") as f:
        for name, url in skipped:
            f.write(f"{name},{url}\n")

    print(f"ğŸ“ è·³è¿‡æ—¥å¿—: {SKIPPED_LOG}")


# ==============================
# ä¸»å…¥å£
# ==============================
if __name__ == "__main__":
    merge_local_sources()